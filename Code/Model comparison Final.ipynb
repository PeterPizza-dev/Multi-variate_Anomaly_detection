{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model comparison Final.ipynb","provenance":[{"file_id":"1CmStSvx9LQ85kbezvjgMKP6uBpX1U5XC","timestamp":1608291651374}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BmBrydJh4mwM"},"source":["# Proposed work flow for training models and comparing them. \n","\n","Each model has been trained on the same training data in separate workbooks, and the trained models have been saved.\n","\n","In this workbook, we import the three trained models along with thresholds, that have been determined on validation set. We have a test set consisting of six sensors, where there are periods of high moisture level, which can be viewed as anomalous. This is converted into a test set, that is passed to each model. The three models are then compared graphically by inspecting where they are able to detect anomalies, and where they fail.\n"]},{"cell_type":"markdown","metadata":{"id":"P6eYOZLZcm_x"},"source":["# Initialization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0CrYn5Ar0T7","executionInfo":{"status":"ok","timestamp":1608561965022,"user_tz":-60,"elapsed":5304,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}},"outputId":"6d7471a3-b7eb-4604-cf16-4718ec2883de"},"source":["!pip install einops"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting einops\n","  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n","Installing collected packages: einops\n","Successfully installed einops-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONnBe3x2cjJG","executionInfo":{"status":"ok","timestamp":1608561968347,"user_tz":-60,"elapsed":8484,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}},"outputId":"4a8ccaa8-1e54-4b16-905e-b4844dd157c2"},"source":["import io\n","import json\n","import os\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import math\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","import random\n","\n","import copy\n","\n","from einops import rearrange\n","\n","import seaborn as sns\n","%pylab inline\n","\n","##Ignoring warnings for now\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['random', 'copy']\n","`%matplotlib` prevents importing * from pylab and numpy\n","  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zbhyPoeedD5-","executionInfo":{"status":"ok","timestamp":1608561985948,"user_tz":-60,"elapsed":26084,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}},"outputId":"eb3121bb-0e4e-4f68-ec38-30ea588464f3","colab":{"base_uri":"https://localhost:8080/"}},"source":["%%capture\n","!pip install pandas matplotlib google-cloud-storage\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Point environment variable `GOOGLE_APPLICATION_CREDENTIALS` to \n","# location of service account file 'dtu-course-02456-students.json'.\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/content/drive/My Drive/Woodsense/Tech/Software/Deep Learning Course DTU/Students Folder/gcp-service-accounts/dtu-course-02456-students.json\"\n","#os.environ['GOOGLE_APPLICATION_CREDENTIALS']  = \"/content/drive/My Drive/Colab Notebooks/02456-deep-learning-with-PyTorch/WoodSense/gcp-service-accounts/dtu-course-02456-students.json\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n","\n","Enter your authorization code:\n","4/1AY0e-g4DkhmRAhLeMFVUiVOq6ssX0O1qjRDuUNPj1QBFuCK25Mnmg79JLE0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jqm68vkVdRI_"},"source":["# Defining functions for preprocessing"]},{"cell_type":"markdown","metadata":{"id":"bQMj4m_KMMyq"},"source":["The following functions are used to convert the test data set into a format that an be used by the models. This includes encoding of the timestamp, normalization, and converting it to samples holding data for 24 hour sequences."]},{"cell_type":"code","metadata":{"id":"FjDe9fxSdTSQ","executionInfo":{"status":"ok","timestamp":1608561985957,"user_tz":-60,"elapsed":26092,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQF2_TPzMu67"},"source":["The GRU Auto-Encoder takes 24 hour sequences and reconstructs the same sequences. The periods are not overlapping, and they always start at midnight."]},{"cell_type":"code","metadata":{"id":"iMOb7IHWdafv","executionInfo":{"status":"ok","timestamp":1608561985957,"user_tz":-60,"elapsed":26090,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["def create_inout_sequences_GRU(input_data, tw, target_size = 1, \n","                                step_size = 1,moisture=True,\n","                                start_from_midnight=True,\n","                                mean_and_std = None):\n","    '''Function that generates sequences based on the current time steps\n","    input_data = The unsctructured data from Woodsense\n","    tw = timewindow/timestemp we want to look back on\n","    target_size = how many hours we want to predict // For anomaly detection this would most likely be 1\n","    step_size = How many steps the window takes over the time series\n","    moisture = Choose moisture or ohms. If moisture = True then ohms is removed and vice versa. \n","\n","    ! Note: \n","    mean_and_std is only applicable if data is standardised with global mean and standard deviation. \n","    '''\n","    inout_seq = []\n","    label = []\n","    timestamps = []\n","    train = []\n","    sensor_list = []\n","\n","    data, mean_and_std = standardise(input_data,mean_and_std)\n","    #print('standard done')\n","\n","    for sensors in input_data['sensor_id'].unique():\n","      data = input_data.loc[input_data['sensor_id'] == sensors]\n","      #print('in loop')\n","\n","      if start_from_midnight:\n","        data = remove_before_00(data)\n","\n","      #print('from midnigt done')\n","\n","      timestamp = data.timestamp.astype(int).to_numpy()#.tolist()\n","      \n","      data = data.drop(columns=['timestamp'])\n","\n","      #print('timestamp removed')\n","\n","      first = data.iloc[0]\n","      last = data.iloc[-1]\n","      L = len(data)\n","\n","      data = data.drop(columns=[ 'sensor_id'])\n","      features = data.columns.tolist()\n","      data = data.astype(np.float32).to_numpy().tolist()\n","      data = torch.FloatTensor(data)\n","      for i in range(24,L-tw-target_size, step_size):\n","        #print('window making begun')\n","        train_seq = data[i:i+tw]\n","        #label_seq = data[i+tw:i+tw+target_size]\n","        timestamp_seq = timestamp[i:i+tw]\n","        #print('sequences made')\n","        train.append(train_seq)\n","        #label.append(label_seq)\n","        timestamps.append(timestamp_seq)\n","        #inout_seq.append((train_seq ,timestamps))\n","        sensor_list += [sensors]\n","    return train, timestamps, sensor_list, mean_and_std, features"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_ap4cU9NGWq"},"source":["For the DeepAnT model, we create samples for every 25 hour sequence (24 hours as input and 1 hour as target value), and these sequences do overlap."]},{"cell_type":"code","metadata":{"id":"Uj5t1wtF7Pp4","executionInfo":{"status":"ok","timestamp":1608561985958,"user_tz":-60,"elapsed":26090,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["def create_inout_sequences_deepant(input_data, tw, target_size = 1, step_size = 1, \n","                                   keep_sensor_id = False, \n","                                   mean_and_std = None):\n","    '''Function that generates sequences based on the current time steps\n","    input_data = The unsctructured data from Woodsens \n","    tw = timewindow/timestemp we want to look back on\n","    target_size = how many hours we want to predict // For anomaly detection this would most likely be 1\n","    '''\n","    inout_seq = []\n","    label = []\n","    train = []\n","    for sensors in input_data['sensor_id'].unique():\n","      data = input_data[input_data['sensor_id'] == sensors]\n","      data = remove_before_00(data)\n","      L = len(data)\n","      timestamps = data['timestamp']\n","      data = data.drop(['timestamp', 'sensor_id'], axis=1)\n","      data, mean_and_std = standardise(data,mean_and_std)\n","      #print(data.isna().sum())\n","      label_data = data.drop(['tod_sin', 'tod_cos', 'doy_sin', 'doy_cos', 'weather_humidity', 'weather_pressure','weather_temp_dew', 'weather_temp_dry','weather_precip_past10min', 'weather_wind_max', 'weather_wind_speed'], axis=1)\n","      data = data.astype(np.float32).to_numpy().tolist()\n","      data = torch.FloatTensor(data)\n","      label_data = label_data.astype(np.float32).to_numpy().tolist()\n","      label_data = torch.FloatTensor(label_data)\n","      for i in range(0,L-tw-target_size, step_size):\n","        train_seq = data[i:i+tw]\n","        train_label = label_data[i+tw:i+tw+target_size]\n","        timestamp = timestamps[i+tw:i+tw+target_size]\n","        label.append(train_label)\n","        train.append(train_seq)\n","        if keep_sensor_id == False:\n","          inout_seq.append((train_seq ,train_label))\n","        else:\n","          inout_seq.append((sensors, timestamp, train_seq ,train_label))\n","    return train, label, inout_seq"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yf_x2r1tNu2t"},"source":["For the Transformer model, we create samples for every 25 hour sequence (24 hours as input and 1 hour as target value), and these sequences do overlap."]},{"cell_type":"code","metadata":{"id":"jd7jGSvRNsEN","executionInfo":{"status":"ok","timestamp":1608561985958,"user_tz":-60,"elapsed":26089,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["def create_inout_sequences_transformer(input_data, tw, target_size = 1, \n","                            step_size = 1,moisture=True,\n","                            start_from_midnight=True,std_on_sensor=False,\n","                            mean_and_std = None):\n","    '''Function that generates sequences based on the current time steps\n","    input_data = The unsctructured data from Woodsense\n","    tw = timewindow/timestemp we want to look back on\n","    target_size = how many hours we want to predict // For anomaly detection this would most likely be 1\n","    step_size = How many steps the window takes over the time series\n","    moisture = Choose moisture or ohms. If moisture = True then ohms is removed and vice versa. \n","\n","    ! Note: \n","    mean_and_std is only applicable if data is standardised with global mean and standard deviation. \n","    '''\n","    inout_seq = []\n","    label = []\n","    timestamps = []\n","    train = []\n","    tgt = []\n","    true = []\n","    sensor_list = []\n","    print('Number of sensors: ',len(input_data['sensor_id'].unique()))\n","\n","    #if moisture:\n","    #  input_data = input_data.drop(columns=['ohms'])\n","    #else:\n","    #  input_data = input_data.drop(columns=['moisture'])\n","\n","    if std_on_sensor is False:\n","      data, mean_and_std = standardise(input_data,mean_and_std)\n","\n","    for sensors in input_data['sensor_id'].unique():\n","      data = input_data.loc[input_data['sensor_id'] == sensors]\n","\n","      if start_from_midnight:\n","        data = remove_before_00(data)\n","\n","      timestamp = data.timestamp.astype(int).to_numpy()#.tolist()\n","      \n","      data = data.drop(columns=['timestamp'])\n","\n","      first = data.iloc[0]\n","      last = data.iloc[-1]\n","      L = len(data)\n","      if std_on_sensor:\n","        data, mean_and_std = standardise(data,mean_and_std)\n","      data = data.drop(columns=[ 'sensor_id'])\n","      features = data.columns.tolist()\n","      data = data.astype(np.float32).to_numpy().tolist()\n","      data = torch.FloatTensor(data)\n","      for i in range(0,L-tw-target_size, step_size):\n","\n","        train_seq = data[i:i+tw-1]\n","        tgt_seq = data[i+tw-2:i+tw-1+target_size]\n","        true_seq = data[i+tw-1:i+tw+target_size]\n","\n","        assert train_seq[-1,2] == tgt_seq[0,2]\n","        assert tgt_seq[-1,2] == true_seq[-2,2]\n","\n","        timestamp_seq = timestamp[i:i+tw+target_size]\n","        #print(len(timestamp_seq))\n","        train.append(train_seq)\n","        tgt.append(tgt_seq)\n","        true.append(true_seq)\n","        timestamps.append(timestamp_seq)\n","        sensor_list += [sensors]\n","    return train, tgt, true, timestamps,  sensor_list, mean_and_std, features"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MKnhPfUcNaqB"},"source":["Standardise data using the mean and standard deviation, such that the data will have zero mean and a standard deviation fo 1 after standardizing."]},{"cell_type":"code","metadata":{"id":"2gJpq9wkhaK2","executionInfo":{"status":"ok","timestamp":1608561985959,"user_tz":-60,"elapsed":26089,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["def standardise(data,mean_and_stds=None):\n","  \"\"\"\n","  Standardises each column in the data. \n","\n","  Parameters:\n","  data (DataFrame): Data object to standardize. Columns must not contain timestamps or strings.\n","\n","  \"\"\"\n","\n","  if mean_and_stds is None:\n","\n","    mean_and_stds = pd.DataFrame()\n","    measures = []\n","    means = []\n","    stds = []\n","    for col in data.columns: \n","      if col not in ['timestamp','sensor_id','tod_sin','tod_cos','doy_sin','doy_cos']:\n","        mean = data[col].mean()\n","        std = data[col].std()\n","\n","        measures += [col]\n","        means += [mean]\n","        stds += [std]\n","        data.loc[:,col] = (data[col]-mean)/std\n","\n","    mean_and_stds['measure'] = measures\n","    mean_and_stds['mean'] = means\n","    mean_and_stds['std'] = stds\n","  \n","  else: \n","\n","    for m in mean_and_stds['measure'].unique():\n","      mean = mean_and_stds.loc[mean_and_stds['measure'] == m, 'mean'].to_numpy()\n","      std = mean_and_stds.loc[mean_and_stds['measure'] == m, 'std'].to_numpy()\n","      data.loc[:,m] = (data.loc[:,m]- mean)/std\n","\n","  return data, mean_and_stds"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPpdQW5fN2MV"},"source":["For the GRU Auto-Encoder, each period will start at midnight. This function removes all timestamps prior to midnight at the beginning of the time series for each sensor. That is, if sensor 20 begins at 15:00, then the data from 15:00 to 23:00 will be removed."]},{"cell_type":"code","metadata":{"id":"Rba5MPFOh0d4","executionInfo":{"status":"ok","timestamp":1608561985959,"user_tz":-60,"elapsed":26087,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["def remove_before_00(df):\n","  \"\"\"\n","  Function removes all the first datarows which measurements are before 00 AM. The function is meant to work \n","  in concurrence with create_inout_sequences per sensor to ensure the time-windows start from 00 AM. \n","\n","  Parameters:\n","  df (DataFrame): DataFrame grouped on a specific sensor\n","\n","  \"\"\"\n","  df['hour'] = df.timestamp.dt.hour\n","  df['day'] = df.timestamp.dt.dayofyear\n","  df['month'] = df.timestamp.dt.month\n","\n","  min_month = df.month.min()\n","  min_day_in_min_month = df[df.month == min_month].day.min()\n","  max_month = df.month.max()\n","  max_day_in_max_month= df[df.month == max_month].day.max()\n","\n","  df = df.sort_values(['timestamp'])\n","\n","  if (df.head(1)['hour'].values == 0):\n","    pass\n","  else:\n","    df = df[(df.month >= min_month) & (df.day >= min_day_in_min_month+1)]\n","\n","  if (df.tail(1)['hour'].values == 0):\n","    pass\n","  else:\n","    #Find first zero: \n","    df = df[(df.day != max_day_in_max_month)]\n","\n","  return df.drop(columns=['hour','day','month'])\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"fj1BvXhQdwJf","executionInfo":{"status":"ok","timestamp":1608561985959,"user_tz":-60,"elapsed":26086,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["class DatasetAE(torch.utils.data.Dataset):\n","  \"\"\"\n","  DataLoader is meant for AutoEncoders\n","  \"\"\"\n","\n","  def __init__(self,data,timestamps,sensors):\n","    super(DatasetAE, self).__init__()\n","\n","    self.data = data\n","    self.timestamps = timestamps\n","    #print(len(timestamps[0]))\n","    self.sensors = sensors\n","\n","  def __len__(self):\n","    return len(self.data)\n"," \n","  def __getitem__(self,idx):\n","    d = self.data[idx]\n","    t = self.timestamps[idx]\n","    s = self.sensors[idx]\n","\n","    return d, t, np.array([s])"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TRbWJlIeON0u"},"source":["We define classes for each model, which can be used with a dataloader."]},{"cell_type":"code","metadata":{"id":"7vDy1LHFAx1E","executionInfo":{"status":"ok","timestamp":1608561985960,"user_tz":-60,"elapsed":26086,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["class Dataset_deepant(torch.utils.data.Dataset):\n","  \"\"\"\n","  DataLoader is meant for DeepAnT\n","  \"\"\"\n","  def __init__(self, data, labels):\n","        'Initialization'\n","        self.labels = labels\n","        self.data = data\n","\n","  def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.data)\n","\n","  def __getitem__(self, index):\n","        'Generates one sample of data'\n","        # Load data and get label\n","        X = self.data[index]\n","        y = self.labels[index]\n","\n","        return X, y"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJQpM5Sqsd8D","executionInfo":{"status":"ok","timestamp":1608561985960,"user_tz":-60,"elapsed":26085,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["class DatasetTransformer(torch.utils.data.Dataset):\n","  \"\"\"\n","  DataLoader is meant for AutoEncoders\n","  \"\"\"\n","\n","  def __init__(self,data,tgt,true,timestamps,sensors):\n","    super(DatasetTransformer, self).__init__()\n","\n","    self.data = data\n","    self.tgt = tgt\n","    self.true = true\n","    self.timestamps = timestamps\n","    #print(len(timestamps[0]))\n","    self.sensors = sensors\n","\n","  def __len__(self):\n","    return len(self.data)\n"," \n","  def __getitem__(self,idx):\n","    d = self.data[idx]\n","    tgt = self.tgt[idx]\n","    true = self.true[idx]\n","    t = self.timestamps[idx]\n","    s = self.sensors[idx]\n","\n","    return d, tgt, true, t, np.array([s])"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXdd1qGreVzh"},"source":["# Preprocessing data for each model"]},{"cell_type":"markdown","metadata":{"id":"VveMjLgy7zML"},"source":["Load table with mean and standard deviation of training set, and load the test data set."]},{"cell_type":"code","metadata":{"id":"TxytDEX_egIq","executionInfo":{"status":"ok","timestamp":1608561988838,"user_tz":-60,"elapsed":28961,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["# Loading mean and std. dev. for each feature in training set to standardise test set\n","\n","mean_and_std = pd.read_csv('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/mean_and_std_train.csv',index_col=False,delimiter=';')\n","\n","#Loading test data (5 different sensors)\n","df = pd.read_csv('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/model_eval_data.csv',index_col=False,delimiter=';')\n","df['timestamp'] = pd.to_datetime(df['timestamp'],infer_datetime_format=True)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Xizs9Y1eYsa"},"source":["## GRU Auto-Encoder Data loader "]},{"cell_type":"markdown","metadata":{"id":"CRSLITRt8T8B"},"source":["Create dataset and data loader to be used with GRU auto-encoder"]},{"cell_type":"code","metadata":{"id":"t0oz90QRfAS2","executionInfo":{"status":"ok","timestamp":1608561988839,"user_tz":-60,"elapsed":28961,"user":{"displayName":"Emil Lindegaard","photoUrl":"","userId":"13245122514177459100"}}},"source":["tw = 24\n","step_size = 24\n","batch_size = 64\n","shuffle = False\n","\n","test_ae, timestamps_ae, sensors_ae, mean_and_std, features = create_inout_sequences_GRU(input_data = df, tw = tw, target_size = 1, \n","                                                                                      step_size = 24, moisture=True,\n","                                                                                      start_from_midnight=True,\n","                                                                                       mean_and_std = mean_and_std)\n","\n","test_ae = DatasetAE(test_ae,timestamps_ae,sensors_ae)\n","\n","AE_loader = DataLoader(test_ae,batch_size=batch_size, shuffle=shuffle, num_workers=8)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8sdD7OnCqhPF"},"source":["## DeepAnT data loader"]},{"cell_type":"markdown","metadata":{"id":"4-wedt3w8u4n"},"source":["Create dataset and data loader to be used with CNN DeepAnT model"]},{"cell_type":"code","metadata":{"id":"DPWPg2VE8NhE"},"source":["mean_and_std = pd.read_csv('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/mean_and_std_train.csv',index_col=False,delimiter=';')\n","\n","#Loading test data (5 different sensors)\n","df = pd.read_csv('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/model_eval_data.csv',index_col=False,delimiter=';')\n","df['timestamp'] = pd.to_datetime(df['timestamp'],infer_datetime_format=True)\n","\n","# sequences for training\n","window = 24\n","test_seq_deepant, test_label_deepant, test_inout_seq_deepant = create_inout_sequences_deepant(input_data = df, \n","                                                                                              tw=window, \n","                                                                                              target_size = 1, \n","                                                                                              step_size = 1, \n","                                                                                              keep_sensor_id = True, \n","                                                                                              mean_and_std = mean_and_std)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b3HdgPA5tHL6"},"source":["## Transformer data loader\n"]},{"cell_type":"markdown","metadata":{"id":"zMhKN3Y69Cas"},"source":["Create data set and data loader to be used with transformer model"]},{"cell_type":"code","metadata":{"id":"59gu9iOntWYC"},"source":["tw = 24\n","step_size = 1\n","target_size = 1\n","batch_size = 128\n","shuffle=False\n","\n","\n","src_tf, tgt_tf, true_tf, timestamps_tf, sensors_tf, mean_and_std, features = create_inout_sequences_transformer(df, tw = tw, target_size = 1, \n","                                                                                      step_size = step_size, \n","                                                                                      start_from_midnight=True,\n","                                                                                       mean_and_std = mean_and_std)\n","\n","test_tf = DatasetTransformer(src_tf,tgt_tf,true_tf,timestamps_tf,sensors_tf)\n","\n","TF_loader = DataLoader(test_tf,batch_size=batch_size, shuffle=shuffle, num_workers=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKBd5xHQql09"},"source":["# Loading models"]},{"cell_type":"markdown","metadata":{"id":"_HxUCoUcsRBY"},"source":["## Load GRU Auto-Encoder"]},{"cell_type":"markdown","metadata":{"id":"1ihTSsa4-AHk"},"source":["The GRU auto-encoder consists of three parts: an encoder, a decoder, and a RecurrentAutoEncoder, which combines the encoder and decoder.  All the classes must be defined before loading the model.\n","\n"]},{"cell_type":"code","metadata":{"id":"qiCH-uCAt5Fx"},"source":["class Encoder(nn.Module):\n","\n","  def __init__(self, seq_len, n_features_in, embedding_dim=64,dropout=0.99,num_layers=1,bidirectional=True):\n","    super(Encoder, self).__init__()\n","    print(dropout)\n","    if bidirectional:\n","      self.num_directions = 2\n","    else:\n","      self.num_directions = 1\n","\n","    self.seq_len =  seq_len\n","    self.n_features = n_features_in\n","    self.embedding_dim = embedding_dim\n","    self.hidden_dim1 = 2 * embedding_dim \n","    self.hidden_dim2 = int((2 * embedding_dim)*self.num_directions)\n","\n","    self.rnn1 = nn.GRU(\n","      input_size=self.n_features,\n","      hidden_size= self.hidden_dim1,\n","      num_layers=num_layers,\n","      batch_first=True,\n","      dropout = 0,\n","      bidirectional = bidirectional\n","    )\n","\n","    self.rnn2 = nn.GRU(\n","      input_size=self.hidden_dim2,\n","      #hidden_size= int(embedding_dim/self.num_directions),\n","      hidden_size = embedding_dim,\n","      num_layers=num_layers,\n","      batch_first=True,\n","      dropout = 0,\n","      bidirectional = bidirectional\n","    )\n","  \n","\n","  def forward(self, x):\n","\n","    x, _ = self.rnn1(x)\n","\n","    x,hidden_n = self.rnn2(x)\n","\n","    hidden_n = hidden_n.permute(1,0,2)\n","\n","    return hidden_n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ExzaC1sPuOKj"},"source":["class Decoder(nn.Module):\n","\n","  def __init__(self, seq_len, input_dim=64, n_features_out=3,dropout=0.99,num_layers=1,bidirectional=False):\n","    super(Decoder, self).__init__()\n","    print(dropout)\n","    if bidirectional:\n","      self.num_directions = 2\n","    else:\n","      self.num_directions = 1\n","\n","    self.seq_len = seq_len\n","    self.input_dim1 = input_dim\n","    self.hidden_size1 = input_dim\n","\n","    self.input_dim2 = self.hidden_size1 * self.num_directions\n","    self.hidden_size2 = int(2*self.input_dim2/self.num_directions)\n","\n","    self.n_features = n_features_out\n","    self.num_layers = num_layers\n","    \n","\n","    self.rnn1 = nn.GRU(\n","      input_size=input_dim,\n","      hidden_size= self.hidden_size1,\n","      num_layers=num_layers,\n","      batch_first=True,\n","      dropout = 0,\n","      bidirectional = bidirectional\n","    )\n","\n","    self.rnn2 = nn.GRU(\n","      input_size= self.input_dim2,\n","      hidden_size= self.hidden_size2,\n","      num_layers=num_layers,\n","      batch_first=True,\n","      dropout = 0,\n","      bidirectional = bidirectional\n","    )\n","\n","    \n","    self.output_layer = nn.Linear(self.hidden_size2*self.num_directions, n_features_out)\n","\n","    self.repetitions = int(self.seq_len/(self.num_layers*self.num_directions))\n","\n","  def forward(self, x):\n","\n","    x = x.repeat(1,self.repetitions, 1)\n","\n","    x, hidden_n = self.rnn1(x)\n","\n","    x, hidden_n = self.rnn2(x)\n","\n","    return self.output_layer(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EXgoZmEuQSN"},"source":["class RecurrentAutoencoder(nn.Module):\n","\n","  def __init__(self, seq_len,n_features_in, n_features_out, embedding_dim=64,dropout=0,num_layers=1,bidirectional=False):\n","    super(RecurrentAutoencoder, self).__init__()\n","\n","    self.encoder = Encoder(seq_len=seq_len, n_features_in=n_features_in, embedding_dim = embedding_dim,\n","                           dropout=dropout,num_layers=num_layers,bidirectional=bidirectional).to(device)\n","    self.decoder = Decoder(seq_len=seq_len, input_dim = embedding_dim, n_features_out = n_features_out,\n","                           dropout=dropout,num_layers=num_layers,bidirectional=bidirectional).to(device)\n","\n","  def forward(self, x):\n","    x = self.encoder(x)\n","    x = self.decoder(x)\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WbtTOEXw-XBq"},"source":["The optimized model is loaded from file."]},{"cell_type":"code","metadata":{"id":"1jKnUAyuqpZV"},"source":["gru_ae = torch.load('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/GRU_AE_epoch_1000.pt', map_location=torch.device(device))\n","gru_ae.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7maHd_XBy4q0"},"source":["## Load DeepAnT model"]},{"cell_type":"markdown","metadata":{"id":"3inMWnVh-6Kx"},"source":["The DeepAnT model is defined as class Network_2layers, which is first defined:"]},{"cell_type":"code","metadata":{"id":"vlX2MQefFxIR"},"source":["class Network_2layers(nn.Module):\n","    def __init__(self, features=14, tw=24, target_features=3, filters1=32, filters2=32, kernel1=3, kernel2=3):\n","        super(Network_2layers, self).__init__()\n","        padding1 = int(kernel1/2)\n","        padding2 = int(kernel2/2)\n","        dimensions_conv_out = int(tw/(2*2))\n","        self.convolutional = nn.Sequential(\n","                nn.Conv1d(features, filters1, kernel1, stride=1, padding=padding1), # 32x24\n","                #nn.Dropout2d(0.5), #50 % probability \n","                nn.MaxPool1d(2, stride=2), #32x12 \n","                nn.ReLU(),\n","                nn.Conv1d(filters1, filters2, kernel2, stride=1, padding=padding2), #32x12\n","                #nn.Dropout2d(0.2), #50 % probability \n","                nn.MaxPool1d(2, stride=2),    # 32 x 6 \n","                nn.ReLU()\n","        )\n","        self.fully_connected = nn.Sequential(\n","                nn.Linear(filters2*dimensions_conv_out, target_features),\n","                #nn.Dropout(p=0.2),\n","                #nn.ReLU(),\n","                #nn.Linear(50, 4)\n","        )\n","    def forward(self, x):\n","        x = torch.transpose(x, 2, 1)\n","        x = self.convolutional(x)\n","        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n","        x = x.view(x.size(0), -1)\n","        x = self.fully_connected(x)\n","\n","        x_shape = x.shape\n","        #print(x_shape)\n","        x = torch.reshape(x, (x_shape[0], 1, x_shape[1]) )\n","        #print(x.shape)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GPpX4vy3_DPy"},"source":["The optimized DeepAnT model is loaded along with the found thresholds."]},{"cell_type":"code","metadata":{"id":"HO2k0cv9LFKO"},"source":["# Load model\n","path = '/content/drive/My Drive/WoodSense/notebooks/Stine/'\n","name = 'DeepANT_final_version3_epoch_100.pt_epoch_100.pt'\n","deepant_model = torch.load(path+name, map_location=torch.device(device))\n","\n","# define networks and parameters\n","net_deepant = deepant_model[0]\n","deepant_threshold = deepant_model[1][0]\n","deepant_param_thresholds = deepant_model[1][1]\n","net_deepant"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1oBKjMDjwXPm"},"source":["## Load Transformer model"]},{"cell_type":"markdown","metadata":{"id":"2m90R-Ev_aTN"},"source":["The transformer model consists of two classes: PositionalEncoding and OutlierTransformer. These are defined before loading the model."]},{"cell_type":"code","metadata":{"id":"KIkETldLwh6o"},"source":["#Create original positional encoding\n","#Source: https://pytorch.org/tutorials/beginner/transformer_tutorial\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=100):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","        \n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nv2tbG0jwsst"},"source":["class OutlierTransformer(nn.Module):\n","    def __init__(self, n_features_in,n_features_out, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, \n","                 max_seq_length, pos_dropout, trans_dropout,pos_encoding=True):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.pos_encoding = pos_encoding\n","        self.embed_src = nn.Linear(n_features_in,d_model)\n","        self.embed_tgt = nn.Linear(n_features_in,d_model) # Needs to be if we need to predict more than 1 timepoint\n","        self.embed = nn.Linear(n_features_in,d_model)\n","\n","        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n","\n","        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n","        #self.fc = nn.Linear(d_model, n_features_out)\n","\n","        self.linear_dropout = nn.Dropout(trans_dropout)\n","\n","        self.fc = nn.Linear(d_model, n_features_out)\n","\n","    def forward(self, src, tgt,tgt_mask=None):\n","\n","        src = rearrange(src, 'n s e -> s n e')\n","        tgt = rearrange(tgt, 'n t e -> t n e')\n","\n","        src = self.embed_src(src) * math.sqrt(self.d_model)\n","        tgt = self.embed_tgt(tgt) * math.sqrt(self.d_model)\n","\n","        if self.pos_encoding:\n","          src = self.pos_enc(src)\n","          tgt = self.pos_enc(tgt)\n","\n","        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n","\n","        output = rearrange(output, 't n e -> n t e')\n","        self.linear_dropout(output)\n","        \n","        output = self.fc(output)\n","        #print(output.shape)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-RY53i8AA4w"},"source":["The optimized transformer model is loaded from file."]},{"cell_type":"code","metadata":{"id":"vjf9nYagwvrA"},"source":["transformer = torch.load('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/Transformer_final_weather_epoch_100.pt', map_location=torch.device(device))\n","transformer.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1SOg7wNy-V3"},"source":["# Running test data through models"]},{"cell_type":"markdown","metadata":{"id":"Q4Y0Zm5-AHlf"},"source":["To compare the models, we run the test data through each of the model to compute a prediction or reconstruction. For each model, we make a data frame with the actual sensor data along with predicted/reconstructed data, the associated errors on each parameter, and the overall Euclidean distance."]},{"cell_type":"markdown","metadata":{"id":"WKRKYsJxzL02"},"source":["## Functions to create dataframes\n"]},{"cell_type":"markdown","metadata":{"id":"0Ja2zqbchBhc"},"source":["First, we define a couple of functions, which will be used to easily compute the predictions and losses, and to create the data frames needed to make plots.\n","\n","First we define a function to compute reconstructions for the GRU auto-encoder."]},{"cell_type":"code","metadata":{"id":"MwRP7OC41Lon"},"source":["def test_loss_calculator_AE(model,test_loader,Loss_type = 'L2',only_sensor_labels=True):\n","\n","  if Loss_type == 'L2':\n","    criterion = nn.MSELoss(reduction='mean').to(device)\n","  elif Loss_type == 'L1':\n","    criterion = nn.L1Loss(reduction='mean').to(device)\n","  else:\n","    print('Choose either L1 or L2 in Loss_type')\n","    return\n","\n","  val_losses = []\n","\n","  true_seqs = []\n","  pred_seqs = []\n","  timesteps = []\n","  sensors = []\n","\n","  model = model.eval()\n","  with torch.no_grad():\n","    for batch in test_loader:\n","      seq_true, timestep, sensor = batch\n","\n","      seq_true = seq_true.to(device)\n","      seq_pred = model(seq_true)\n","\n","      if only_sensor_labels:\n","        seq_true = seq_true[:,:,0:3]\n","\n","      loss = criterion(seq_pred, seq_true)\n","\n","      val_losses.append(loss.item())\n","\n","      seq_true = seq_true.cpu().numpy()\n","      seq_pred = seq_pred.cpu().numpy()\n","\n","      true_seqs += [seq_true]\n","      pred_seqs += [seq_pred]\n","      timesteps += [timestep.numpy()]\n","      sensors += [sensor.numpy()]\n","\n","    return np.mean(val_losses), true_seqs, pred_seqs, timesteps, sensors\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnTsmny4hyPH"},"source":["Next, we make a function to compute the predictions and losses for the transformer model."]},{"cell_type":"code","metadata":{"id":"je8yWTFxx3tZ"},"source":["def test_loss_calculator_transformer(model,test_loader,Loss_type = 'L2',only_sensor_labels=True):\n","\n","  if Loss_type == 'L2':\n","    criterion = nn.MSELoss(reduction='mean').to(device)\n","  elif Loss_type == 'L1':\n","    criterion = nn.L1Loss(reduction='mean').to(device)\n","  else:\n","    print('Choose either L1 or L2 in Loss_type')\n","    return\n","\n","  val_losses = []\n","  last_val_losses = []\n","\n","  true_seqs = []\n","  pred_seqs = []\n","  timesteps = []\n","  sensors = []\n","\n","\n","  \n","  model = model.eval()\n","  with torch.no_grad():\n","    for batch in test_loader:\n","      src,tgt,true_tgt,time, sensor = batch\n","\n","      src = src.to(device)\n","      tgt = tgt.to(device)\n","      true_tgt = true_tgt[:,-1:,:3].to(device) # First 3 as targets to predict\n","\n","      tgt_mask = model.transformer.generate_square_subsequent_mask(tgt.size(1)).to(device)\n","\n","      pred_tgt = model(src,tgt,tgt_mask)\n","      pred_tgt = pred_tgt[:,-1:,:3]\n","\n","      loss = criterion(pred_tgt, true_tgt)\n","      #loss_last = criterion_last(pred_tgt[:,-1,:],true_tgt[:,-1,:])\n","      #print(loss)\n","      true_seqs.append(true_tgt.cpu().numpy())\n","      pred_seqs.append(pred_tgt.cpu().numpy())\n","      timesteps.append(time.numpy()[:,-1])\n","      sensors.append(sensor.numpy().squeeze())\n","      val_losses.append(loss.item())\n","\n","    return np.mean(val_losses), true_seqs, pred_seqs, timesteps, sensors\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z4ZgGFZGiC6V"},"source":["The next function is used to convert the sequences of reconstructions, true values, timestamps, and sensor id and convert it into a data frame for the GRU auto-encoder. "]},{"cell_type":"code","metadata":{"id":"YFbM3MiEzB0p"},"source":["#Creating dataframe holding both L1 and L2 loss for each sample\n","\n","def create_dataframe_AE(true_seqs,pred_seqs,timestamps,sensors,time_step = 1):\n","\n","  \"\"\"\n","  Creating dataframe holding both L1 and L2 loss for each sample for \n","  both temperature, humidity and moisture for the GRU auto-encoder\n","  \"\"\"\n","\n","\n","  cols = [str(i) + ' - ' + str(i+time_step) for i in range(0,24,time_step)]\n","\n","  measure_dict = {'temperature':0,'humidity':1,'moisture':2}\n","\n","  df = pd.DataFrame({'time-period':[],\n","                          'timestamp':[],\n","                          'sensor':[],\n","                          'true':[],\n","                          'pred':[],\n","                          'L1Loss':[],\n","                          'L2Loss':[]})\n","  \n","  for m_type in ['temperature','humidity','moisture']:\n","    m_idx = measure_dict[m_type]\n","\n","    for true_batch, pred_batch, timestamp, sensor in zip(true_seqs,pred_seqs,timestamps, sensors):\n","      for i,t in enumerate(range(0,24,time_step)):\n","\n","        true_timestep = true_batch[:,t:t+time_step,m_idx].reshape(-1)\n","        pred_timestep = pred_batch[:,t:t+time_step,m_idx].reshape(-1)\n","        \n","        timestep = timestamp[:,t:t+time_step].reshape(-1)\n","\n","        sensor_per_row = np.array([[s]*time_step for s in sensor]).squeeze().reshape(-1)\n","\n","        col_timestep = [cols[i]] * len(true_timestep)\n","\n","        L2_loss = (true_timestep - pred_timestep)**2\n","        L1_loss = np.absolute(true_timestep - pred_timestep)\n","\n","        df_tmp = pd.DataFrame({'time-period':col_timestep,\n","                               'timestamp':timestep,\n","                               'sensor':sensor_per_row,\n","                                'true':true_timestep,\n","                               'pred':pred_timestep,\n","                               'L1Loss':L1_loss,\n","                               'L2Loss':L2_loss})\n","        \n","        df_tmp['measure_type'] = m_type\n","        \n","        df = df.append(df_tmp)\n","\n","  df.timestamp = pd.to_datetime(df.timestamp,infer_datetime_format=True)\n","  df.sensor = df.sensor.astype(int)\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-UTt4D7aj5wo"},"source":["The next function is used to convert the sequences of reconstructions, true values, timestamps, and sensor id and convert it into a data frame for the Transformer model"]},{"cell_type":"code","metadata":{"id":"-EUrsV4N5EX6"},"source":["def create_dataframe_transformer(true_seqs,pred_seqs,timesteps,sensors,time_step = 4,m_type = 'moisture'):\n","  \"\"\"\n","  Creating dataframe holding both L1 and L2 loss for each sample for \n","  both temperature, humidity and moisture for the Transformer model\n","  \"\"\"\n","\n","  cols_start = [str(i) + ' - ' + str(i+time_step) for i in range(0,24,time_step)]\n","  cols = []\n","  for col in cols_start:\n","    cols += [col]*time_step\n","\n","  measure_dict = {'temperature':0,'humidity':1,'moisture':2,'euclidian_all':-1}\n","  m_idx = measure_dict[m_type]\n","\n","  df = pd.DataFrame({'time_periods':[],\n","                     'sensor_id':[],\n","                      'timestamp':[],\n","                      'loss':[],\n","                      'euclidean_dist':[],\n","                      'loss_temp':[],\n","                      'loss_humid':[],\n","                      'loss_moist':[],\n","                      'true_temp':[],\n","                      'true_humid':[],\n","                      'true_moist':[],\n","                      'pred_temp':[],\n","                      'pred_humid':[],\n","                      'pred_moist':[]})\n","\n","\n","  thresholds = pd.read_csv('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/Transformer_threshold_final.csv')\n","\n","  for true_batch, pred_batch, time, sensor in zip(true_seqs,pred_seqs,timesteps,sensors):\n","\n","    measures_true = []\n","    measures_pred = []\n","    measures_loss = []\n","\n","    for m_idx in range(4):\n","\n","      if m_idx == 3:\n","        \n","        euclidian_distance = np.linalg.norm(true_batch - pred_batch,ord=2,axis=2).squeeze()\n","\n","      else:\n","\n","        true_timestep = true_batch[:,:,m_idx].reshape(-1)\n","        pred_timestep = pred_batch[:,:,m_idx].reshape(-1)\n","\n","        measures_true.append(true_timestep)\n","        measures_pred.append(pred_timestep)\n","\n","        time_df = pd.DataFrame()\n","        time_df['time'] = pd.to_datetime(time)\n","        time_df['hour'] = time_df['time'].dt.hour\n","        col_timestep = [cols[i] for i in time_df['hour']]\n","\n","        #print(true_timestep[:10])\n","        #print(pred_timestep[:10])\n","\n","        L1_loss = np.absolute(true_timestep - pred_timestep)\n","        #print(L1_loss[:10])\n","\n","\n","        measures_loss.append(L1_loss)\n","\n","    df_tmp = pd.DataFrame({'time_periods':col_timestep,\n","                        'sensor_id':sensor,\n","                        'timestamp': time_df['time'],\n","                        'euclidean_dist':euclidian_distance,\n","                        'loss_temp':measures_loss[0],\n","                        'loss_humid':measures_loss[1],\n","                        'loss_moist':measures_loss[2],\n","                        'true_temp':measures_true[0],\n","                        'true_humid':measures_true[1],\n","                        'true_moist':measures_true[2],\n","                        'pred_temp':measures_pred[0],\n","                        'pred_humid':measures_pred[1],\n","                        'pred_moist':measures_pred[2]\n","                        })   \n","    \n","    df = df.append(df_tmp)\n","\n","  df['loss'] = df[['loss_temp','loss_humid','loss_moist']].mean(1)\n","  df['threshold_euc'] = thresholds['euclidian_threshold'].to_numpy()[0]\n","  df['threshold_temp'] = thresholds['temperature_threshold'].to_numpy()[0]\n","  df['threshold_humid'] = thresholds['humidity_threshold'].to_numpy()[0]\n","  df['threshold_moist'] = thresholds['moisture_threshold'].to_numpy()[0]\n","  df['sensor_id'] = df['sensor_id'].astype(int)\n","\n","  df.reset_index(inplace=True)\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RCt8T9t2nOG"},"source":["def plot_scatter(true_seqs,pred_seqs,timesteps,sensors,time_step=4,m_type='moisture',figsize=(20,10),alpha=0.8):\n","  \"\"\"\n","  Makes scatter plot of predicted/reconstructed values versus the true values.\n","  \"\"\"\n","  df = create_dataframe_AE(true_seqs,pred_seqs,timesteps,sensors,time_step)\n","  df = df[df.measure_type == m_type]\n","  plt.figure(figsize=figsize)\n","  sns.set_style('whitegrid')\n","  sns.scatterplot(data=df, x=\"true\", y=\"pred\", hue=\"time-period\",alpha=alpha)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iC704l_nkSmv"},"source":["The dataframe created by create_dataframe_AE differs in format from the dataframe created for the DeepAnT model, the transformer model, and the format used in plots. The next function can therefore be used to convert the dataframe to the right format."]},{"cell_type":"code","metadata":{"id":"5ZwWDT4UzmmV"},"source":["# Convert to dataframe layout used in plots\n","def convert_dataframe(df_data, df_thresholds):\n","  '''\n","  This function converts from the dataframe from create_dataframe_AE and returns\n","  the data in a dataframe, that can be used in the plotting functions\n","  '''\n","  df_data = df_data.drop(columns = ['L2Loss'])\n","\n","  df_temp = df_data[df_data['measure_type']=='temperature']\n","  df_temp.columns = ['time-period', 'timestamp', 'sensor_id', 'true_temp', 'pred_temp', 'loss_temp', 'measure_type']\n","  df_temp = df_temp.drop(columns = ['measure_type'])\n","\n","  df_moist = df_data[df_data['measure_type']=='moisture']\n","  df_moist.columns = ['time-period', 'timestamp', 'sensor_id', 'true_moist', 'pred_moist', 'loss_moist', 'measure_type']\n","  df_moist = df_moist.drop(columns = ['measure_type'])\n","\n","  df_humid = df_data[df_data['measure_type']=='humidity']\n","  df_humid.columns = ['time-period', 'timestamp', 'sensor_id', 'true_humid', 'pred_humid', 'loss_humid', 'measure_type']\n","  df_humid = df_humid.drop(columns = ['measure_type'])\n","\n","\n","  new_df = pd.merge(df_temp, df_moist, on=[\"sensor_id\", \"timestamp\", \"time-period\"])\n","  new_df = pd.merge(new_df, df_humid, on=[\"sensor_id\", \"timestamp\", \"time-period\"])\n","\n","  new_df.loc[:,'euclidean_dist'] = sqrt(new_df.loc[:,'loss_temp']**2 + new_df.loc[:,'loss_humid']**2 + new_df.loc[:,'loss_moist']**2)\n","  new_df.loc[:,'loss'] = (new_df.loc[:,'loss_temp'] + new_df.loc[:,'loss_humid'] + new_df.loc[:,'loss_moist'])/3\n","  new_df.sort_values(by=['sensor_id','timestamp'], inplace = True)\n","\n","\n","  # Add thresholds to dataframe\n","  new_df.loc[:, 'threshold_euc'] = 0\n","  new_df.loc[:, 'threshold_temp'] = 0\n","  new_df.loc[:, 'threshold_humid'] = 0\n","  new_df.loc[:, 'threshold_moist'] = 0\n","  for index, row in df_thresholds.iterrows():\n","    new_df.loc[new_df.timestamp.dt.hour == row.timesteps,'threshold_euc'] = row.euclidian_threshold\n","    new_df.loc[new_df.timestamp.dt.hour == row.timesteps,'threshold_temp']= row.temperature_threshold\n","    new_df.loc[new_df.timestamp.dt.hour == row.timesteps,'threshold_humid'] = row.humidity_threshold\n","    new_df.loc[new_df.timestamp.dt.hour == row.timesteps,'threshold_moist'] = row.moisture_threshold\n","\n","  new_df.reset_index(inplace=True)\n","\n","  return new_df\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-trD-Fp2w6A"},"source":["## GRU Auto-Encoder create dataframe"]},{"cell_type":"markdown","metadata":{"id":"sKl6089clL3Y"},"source":["The dataframe is made using the defined functions:"]},{"cell_type":"code","metadata":{"id":"Q0QmenNOzaLK"},"source":["gru_test_loss, true_seqs, pred_seqs, timesteps, sensors = test_loss_calculator_AE(gru_ae,AE_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"potDGiDC2Mr2"},"source":["df_gru_loss = create_dataframe_AE(true_seqs,pred_seqs,timesteps, sensors, time_step=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"411gXl1d2fd0"},"source":["# show top of the dataframe\n","df_gru_loss[0:4 ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUO4uhP1lWTm"},"source":["To get an overview of how the reconstructions are compared to the true values, we make a scatter plot. For low moisture levels, the points are close to diagonal (corresponding to good reconstructions), but for higher moisture levels, the points are more scattered, which indicates that the model is not at good at recreating the data, which could result in detected anomalies."]},{"cell_type":"code","metadata":{"id":"-5a-3Zw72_Gz"},"source":["plot_scatter(true_seqs,pred_seqs,timesteps,sensors,time_step=4,m_type='moisture')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjbhB8sbmKL4"},"source":["The thresholds for the GRU auto-encoder are loaded."]},{"cell_type":"code","metadata":{"id":"NJ85z_yQxuzv"},"source":["gru_thresholds = pd.read_csv('/content/drive/MyDrive/WoodSense/notebooks/Final models and comparison/GRU_AE_threshold.csv',index_col=False,delimiter=',')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wfWeivvmiKS"},"source":["We convert the dataframe to the wanted format for the model comparison using the defined function."]},{"cell_type":"code","metadata":{"id":"IAX0bLRPxTSA"},"source":["# Convert dataframe to format used in plots\n","df_gru_loss_plot = convert_dataframe(df_gru_loss, gru_thresholds) # convert dataframe to the one used in plots\n","df_gru_loss_plot.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lsIpKcczWdz"},"source":["## DeepAnT create dataframe"]},{"cell_type":"markdown","metadata":{"id":"xqD8eEb2mxKJ"},"source":["Next we create the dataframe for the DeepAnT model. This dataframe will have the same format as the one created for the GRU auto-encoder model."]},{"cell_type":"code","metadata":{"id":"41EplJ5NzceC"},"source":["def euclidean_dist(x, y): # Calculates euclidean dist\n","  if (type(x) == float):\n","    return abs(x-y)\n","  return np.sqrt(sum((y-x)**2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnRFF6q90CFJ"},"source":["# Use trained network to predict next points. \n","# Store all values (true values, predictions, losses, thresholds) in a dataframe\n","\n","net_deepant = net_deepant.to(device)\n","combined_data = [];\n","criterion = nn.L1Loss()\n","parameters_predicted = ['temp', 'humid', 'moist']\n","\n","for i in range(len(test_inout_seq_deepant)):\n","  sensor, timestamp, X, y = test_inout_seq_deepant[i]\n","  timestamp = timestamp.iloc[0]\n","  X, y = X.to(device), y.to(device)\n","  with torch.no_grad():\n","    X = X.reshape(1, tw, 14)\n","    output = net_deepant(X)\n","  y = y.reshape(output.shape)\n","  \n","  loss = criterion(y, output) # same loss as used in training\n","  loss = float(loss.cpu().detach().numpy())\n","  output = output.cpu().detach().numpy()\n","  output = np.squeeze(output)  \n","  y = y.cpu().detach().numpy()\n","  y = np.squeeze(y)\n","\n","  if y.size == 1:\n","    euc_dist = abs(y-output);\n","    temp_list = [sensor, timestamp, loss, euc_dist, euc_dist, y, output]\n","  else:\n","    euc_dist = euclidean_dist(y,output) # euclidean distance\n","    temp_list = [sensor, timestamp, loss, euc_dist] # list to hold sensor id, losses, values\n","    \n","    for y_i, output_i in zip(y, output): # iterate through the predicted features\n","      temp_list.append(abs(output_i-y_i))\n","    temp_list.extend(y)\n","    temp_list.extend(output)\n","    \n","  combined_data.append(temp_list)\n","\n","# Convert to dataframe\n","column_names = ['sensor_id', 'timestamp', 'loss', 'euclidean_dist']\n","for i, param in enumerate(parameters_predicted):\n","  column_names.append('loss_' + param)\n","\n","for i, param in enumerate(parameters_predicted):\n","  column_names.append('true_' + param)\n","\n","for i, param in enumerate(parameters_predicted):\n","  column_names.append('pred_' + param)\n","\n","df_test_loss_deepant = pd.DataFrame(combined_data)\n","df_test_loss_deepant.columns = column_names\n","\n","# Add thresholds to dataframe\n","df_test_loss_deepant.loc[:, 'threshold_euc'] = deepant_threshold\n","df_test_loss_deepant.loc[:, 'threshold_temp'] = deepant_param_thresholds[0]\n","df_test_loss_deepant.loc[:, 'threshold_humid'] = deepant_param_thresholds[1]\n","df_test_loss_deepant.loc[:, 'threshold_moist'] = deepant_param_thresholds[2]\n","\n","# Show top of dataframe\n","df_test_loss_deepant.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZMZDMQonL_0"},"source":["We make a scatter plot of the predicted versus true values. We see that moisture levels around 1 result in the best predictions. For lower values and higher values, the predictions seem to be further from the diagonal line."]},{"cell_type":"code","metadata":{"id":"KM4rQ3_PvSf0"},"source":["m_type='moist'\n","figsize=(20,10)\n","alpha = 0.8\n","plt.figure(figsize=figsize)\n","sns.set_style('whitegrid')\n","sns.scatterplot(data=df_test_loss_deepant, x=\"true_\"+m_type, y=\"pred_\"+m_type,alpha=alpha)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_IThVd7o1U_F"},"source":["## Transformer create dataframe"]},{"cell_type":"markdown","metadata":{"id":"uhhJtVBKnjxa"},"source":["We create a dateframe for the transformer model and make a scatter plot of predicted versus true values of moisture."]},{"cell_type":"code","metadata":{"id":"iO5vYH_a1beN"},"source":["tf_test_loss, tf_true_seqs, tf_pred_seqs, tf_timesteps, tf_sensors = test_loss_calculator_transformer(transformer,TF_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKNH3pLT2TwV"},"source":["df_transformer_loss = create_dataframe_transformer(tf_true_seqs,tf_pred_seqs,tf_timesteps,tf_sensors,time_step = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tij7aUGwH4NT"},"source":["df_transformer_loss.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcZsyZkGHoiN"},"source":["  plt.figure(figsize=(20,10))\n","  sns.set_style('whitegrid')\n","  sns.scatterplot(data=df_transformer_loss, x=\"true_moist\", y=\"pred_moist\", hue=\"time_periods\",alpha=0.8)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VFesYuLZDCUq"},"source":["# Plots and model comparison"]},{"cell_type":"markdown","metadata":{"id":"GQcs1QUmtFy5"},"source":["First we convert the data back to the original units using the mean and standard deviation."]},{"cell_type":"code","metadata":{"id":"TMVFyF0uDp2V"},"source":["mean_and_std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRNHhfSpvpLn"},"source":["parameters = ['temp', 'humid', 'moist']\n","df_models = [df_transformer_loss, df_gru_loss_plot, df_test_loss_deepant]\n","for df in df_models:\n","  for index, row in mean_and_std[0:3].iterrows():\n","    param = parameters[index]\n","    mean = row.iloc[1]\n","    std = row.iloc[2] \n","    df['true_'+param] = df['true_'+param]*std + mean\n","    df['pred_'+param] = df['pred_'+param]*std + mean"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yCcpap3_oAq-"},"source":["The true value of moisture and the predictions/reconstructions are shown below for sensor 50."]},{"cell_type":"code","metadata":{"id":"NLF3XPHoDxOm"},"source":["import plotly.graph_objects as go\n","sensor = 50\n","# Create traces\n","fig = go.Figure()\n","\n","\n","\n","fig.add_trace(go.Scatter(y=df_transformer_loss['true_moist'][df_transformer_loss['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='True'))\n","fig.add_trace(go.Scatter(y=df_transformer_loss['pred_moist'][df_transformer_loss['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='Transformer'))\n","fig.add_trace(go.Scatter(y=df_test_loss_deepant['pred_moist'][df_test_loss_deepant['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='DeepANT'))\n","fig.add_trace(go.Scatter(y=df_gru_loss_plot['pred_moist'][df_gru_loss_plot['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='GRU'))\n","#style layout \n","layout = go.Layout(\n","    xaxis=dict(\n","        title=\"t\"\n","    ),\n","    yaxis=dict(\n","        title=\"Moisture [%]\"\n","    ) ) \n","fig.layout = layout\n","\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jEXDVnbJXwp"},"source":["# Outlier dataframe"]},{"cell_type":"markdown","metadata":{"id":"patdsVdut6PS"},"source":["Lasse Regin from Woodsense has provided some time intervals for some different sensors, where the moisture levels appear to be abnormally high. These periods are defined below. It is tested whether the models are able to detect the outliers in this period."]},{"cell_type":"code","metadata":{"id":"Iy8kuFPcJaE6"},"source":["outlier_time = {\n","\"20\": [[\"2020-09-15T00:00:00\", \"2020-09-22T00:00:00\"], [\"2020-10-07T00:00:00\", \"2020-10-16T00:00:00\"]], \n","\"25\": [[\"2020-10-21T00:00:00\", \"2020-11-30T00:00:00\"]], \n","\"26\": [[\"2020-10-20T00:00:00\", \"2020-11-30T00:00:00\"]], \n","\"27\": [[\"2020-10-20T00:00:00\", \"2020-11-30T00:00:00\"]], \n","\"50\": [[\"2020-10-21T00:00:00\", \"2020-11-30T00:00:00\"]], \n","\"51\": [[\"2020-10-19T00:00:00\", \"2020-11-19T00:00:00\"]]\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9_c5Y3IuWVF"},"source":["We want find out how many of the datapoints in this period are detected as anomalies by each model. We make a dataframe that holds each period, the length of the period, and the number of outliers in the period. The column pct_outliers is the percentage of outliers detected in the period. The mean_above_threshold column calculates difference $euc\\_dist-threshold$ for each of the detected outliers and takes the average of this."]},{"cell_type":"code","metadata":{"id":"LfEABSkrJf-j"},"source":["# For each sensor and anomalous period, count the number of outliers, that each model detect\n","df_outliers = pd.DataFrame(columns=df_transformer_loss.columns)\n","for key in outlier_time.keys():\n","  time_period = outlier_time[key]\n","\n","  for model, name in [(df_gru_loss_plot,'GRU'),(df_test_loss_deepant,'CNN'),(df_transformer_loss,'Transformer')]:\n","    for t in time_period:\n","      df_time = model[model.sensor_id == int(key)].copy()\n","      df_time = df_time.set_index(['timestamp']).loc[t[0]:t[1]].reset_index(['timestamp'])\n","\n","      df_time['outlier_period'] = [str(t[0])[:10] + ' - ' + str(t[1])[:10] for i in range(len(df_time))]\n","\n","      df_time['outlier_period'] = 'sensor ' + df_time['sensor_id'].astype(str) + ':  ' + df_time['outlier_period'].astype(str)\n","\n","      df_time['Outlier'] = df_time['euclidean_dist'] > df_time['threshold_euc']\n","\n","      df_time['Model'] = name\n","      df_time['length_of_period'] = int(len(df_time))\n","\n","      df_outliers = df_outliers.append(df_time)\n","\n","df_outliers.drop(columns=['index','time_periods','time-period'])\n","df_outliers['thres_euc_dif'] = df_outliers['euclidean_dist'] - df_outliers['threshold_euc']\n","\n","df_outliers = df_outliers[['Model','sensor_id','timestamp','outlier_period','length_of_period','Outlier','thres_euc_dif','euclidean_dist','threshold_euc','loss',\n","                           'loss_temp','loss_humid','loss_moist','true_temp','true_humid','true_moist',\n","                           'pred_temp','pred_humid','pred_moist','threshold_temp','threshold_humid','threshold_moist']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpGAsmiMJi2Q"},"source":["# Make table that is grouped by model and period\n","df_outliers2 = df_outliers.copy()\n","df_outlier_table = df_outliers2.loc[:,['Model','outlier_period','Outlier','euclidean_dist','length_of_period']].groupby(['Model','outlier_period'])\\\n","                                  .agg(num_outliers = pd.NamedAgg(column='Outlier',aggfunc='sum'),\n","                                      length_of_period = pd.NamedAgg(column='length_of_period',aggfunc='mean'),\n","                                      mean_euclidian_dist = pd.NamedAgg(column='euclidean_dist',aggfunc='mean')\n","                                      #thres_euclidean = pd.NamedAgg(column='threshold_euc',aggfunc='mean')\n","                                      )\n","\n","# Add a column that has the recall in the given period                                  \n","df_outlier_table['pct_outliers'] = df_outlier_table['num_outliers']/df_outlier_table['length_of_period']*100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUBYu-0SJlPc"},"source":["# add a column, which has the average difference between euc. distance and threshold for any detected outlier\n","df_outliers3 = df_outliers[df_outliers.Outlier == True].copy()\n","df_outliers_table_app =     df_outliers3.loc[:,['Model','outlier_period','thres_euc_dif']].groupby(['Model','outlier_period'])\\\n","                                            .agg(mean_above_thres = pd.NamedAgg(column='thres_euc_dif',aggfunc='mean')\n","                                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Z4sYu1gJnuQ"},"source":["# Merge tables\n","df_outlier_table = df_outlier_table.merge(df_outliers_table_app,on=['Model','outlier_period'],how='left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-faGTuNJwMz"},"source":["df_outlier_table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5D_fAb60rwTt"},"source":["We can plot the true values of moisture along with predictions/reconstructiosn from each model. The figure also shows the detected outliers by each model."]},{"cell_type":"code","metadata":{"id":"dDrpUG1h4wLS"},"source":["import plotly.graph_objects as go\n","\n","# Create traces\n","fig = go.Figure()\n","\n","sensor = 25\n","measure = 'moist'\n","\n","## GRU model\n","fig.add_trace(go.Scatter(x=df_gru_loss_plot['timestamp'][df_gru_loss_plot['sensor_id'] == sensor],y=df_gru_loss_plot['true_'+measure][df_gru_loss_plot['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='True moisture'))\n","\n","fig.add_trace(go.Scatter(x=df_gru_loss_plot['timestamp'][df_gru_loss_plot['sensor_id'] == sensor],y=df_gru_loss_plot['pred_'+measure][df_gru_loss_plot['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='GRU AE reconst.'))\n","\n","#GRU outliers\n","df_period_outliers_gru = df_gru_loss_plot.loc[(df_gru_loss_plot.euclidean_dist >= df_gru_loss_plot.threshold_euc) &\n","                                     (df_gru_loss_plot.sensor_id == sensor)][['timestamp','pred_'+measure]]\n","\n","fig.add_trace(go.Scatter(mode=\"markers\", x=df_period_outliers_gru['timestamp'], \n","                         y=df_period_outliers_gru['pred_'+measure], \n","                         name=\"GRU AE outliers\"))\n","\n","#DeepAnt model\n","fig.add_trace(go.Scatter(x=df_test_loss_deepant['timestamp'][df_test_loss_deepant['sensor_id'] == sensor],y=df_test_loss_deepant['pred_'+measure][df_test_loss_deepant['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='DeepAnt Pred.'))\n","\n","#DeepAnt outliers\n","df_period_outliers_deepant = df_test_loss_deepant.loc[(df_test_loss_deepant.euclidean_dist >= df_test_loss_deepant.threshold_euc) &\n","                                     (df_test_loss_deepant.sensor_id == sensor)][['timestamp','pred_'+measure]]\n","\n","fig.add_trace(go.Scatter(mode=\"markers\", x=df_period_outliers_deepant['timestamp'], \n","                         y=df_period_outliers_deepant['pred_'+measure], \n","                         name=\"DeepAnt outliers\"))\n","\n","# Transformer model\n","fig.add_trace(go.Scatter(x=df_transformer_loss['timestamp'][df_transformer_loss['sensor_id'] == sensor],y=df_transformer_loss['pred_'+measure][df_transformer_loss['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='Transformer Pred.'))\n","\n","#Transformer outliers\n","df_period_outliers_transformer = df_transformer_loss.loc[(df_transformer_loss.euclidean_dist >= df_transformer_loss.threshold_euc) &\n","                                     (df_transformer_loss.sensor_id == sensor)][['timestamp','pred_'+measure]]\n","\n","fig.add_trace(go.Scatter(mode=\"markers\", x=df_period_outliers_transformer['timestamp'], \n","                         y=df_period_outliers_transformer['pred_'+measure], \n","                         name=\"Transformer outliers\"))\n","\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-09-15T00:00:00\", y0=5, x1=\"2020-09-15T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-09-22T00:00:00\", y0=5, x1=\"2020-09-22T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-10-07T00:00:00\", y0=5, x1=\"2020-10-07T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-10-15T00:00:00\", y0=5, x1=\"2020-10-15T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","\n","fig.add_annotation(x=\"2020-10-07T00:00:00\", y=25,\n","            text=\"Anomaly period 2\",\n","            showarrow=True,\n","            arrowhead=1,\n","            ax=70,\n","            ay=-50)\n","\n","fig.add_annotation(x=\"2020-09-15T00:00:00\", y=25,\n","            text=\"Anomaly period 1\",\n","            showarrow=True,\n","            arrowhead=1,\n","            ax=60,\n","            ay=-50)\n","\n","#style layout \n","layout = go.Layout(\n","    xaxis=dict(\n","        title=\"Date\"\n","    ),\n","    yaxis=dict(\n","        title=\"Moisture [%]\"\n","    ),\n","    font = dict(size = 18)) \n","fig.layout = layout\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"No3azegaBxF1"},"source":["import plotly.graph_objects as go\n","\n","# Create traces\n","fig = go.Figure()\n","\n","sensor = 20\n","measure = 'moist'\n","\n","## GRU model\n","fig.add_trace(go.Scatter(x=df_gru_loss_plot['timestamp'][df_gru_loss_plot['sensor_id'] == sensor],y=df_gru_loss_plot['true_'+measure][df_gru_loss_plot['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='True '+measure))\n","\n","fig.add_trace(go.Scatter(x=df_gru_loss_plot['timestamp'][df_gru_loss_plot['sensor_id'] == sensor],y=df_gru_loss_plot['pred_'+measure][df_gru_loss_plot['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='GRU Auto-Encoder Reconstrution. '+measure))\n","\n","#GRU outliers\n","df_period_outliers_gru = df_outliers.loc[(df_outliers.Outlier == True) &\n","                                     (df_outliers.sensor_id == sensor) &\n","                                     (df_outliers.Model == 'GRU')][['timestamp','pred_'+measure]]\n","\n","fig.add_trace(go.Scatter(mode=\"markers\", x=df_period_outliers_gru['timestamp'], y=df_period_outliers_gru['pred_'+measure], name=\"GRU outliers\"))\n","\n","#DeepAnt model\n","fig.add_trace(go.Scatter(x=df_test_loss_deepant['timestamp'][df_test_loss_deepant['sensor_id'] == sensor],y=df_test_loss_deepant['pred_'+measure][df_test_loss_deepant['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='DeepAnt Pred .'+measure))\n","\n","#DeepAnt outliers\n","df_period_outliers_deepant = df_outliers.loc[(df_outliers.Outlier == True) &\n","                                     (df_outliers.sensor_id == sensor) &\n","                                     (df_outliers.Model == 'CNN')][['timestamp','pred_'+measure]]\n","\n","fig.add_trace(go.Scatter(mode=\"markers\", x=df_period_outliers_deepant['timestamp'], y=df_period_outliers_deepant['pred_'+measure], name=\"DeepAnt outliers\"))\n","\n","# Transformer model\n","fig.add_trace(go.Scatter(x=df_transformer_loss['timestamp'][df_transformer_loss['sensor_id'] == sensor],y=df_transformer_loss['pred_'+measure][df_transformer_loss['sensor_id'] == sensor],\n","                    mode='lines',\n","                    name='Transformer Pred. '+measure))\n","\n","df_period_outliers_deepant = df_outliers.loc[(df_outliers.Outlier == True) &\n","                                     (df_outliers.sensor_id == sensor) &\n","                                     (df_outliers.Model == 'CNN')][['timestamp','true_'+measure]]\n","\n","#Transformer outliers\n","df_period_outliers_transformer = df_outliers.loc[(df_outliers.Outlier == True) &\n","                                     (df_outliers.sensor_id == sensor) &\n","                                     (df_outliers.Model == 'Transformer')][['timestamp','pred_'+measure]]\n","\n","fig.add_trace(go.Scatter(mode=\"markers\", x=df_period_outliers_transformer['timestamp'], y=df_period_outliers_transformer['pred_'+measure], name=\"Transformer outliers\"))\n","\n","#style layout \n","layout = go.Layout(\n","    xaxis=dict(\n","        title=\"Date\"\n","    ),\n","    yaxis=dict(\n","        title=\"Moisture [%]\"\n","    ) ) \n","fig.layout = layout\n","\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-09-15T00:00:00\", y0=5, x1=\"2020-09-15T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-09-22T00:00:00\", y0=5, x1=\"2020-09-22T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-10-07T00:00:00\", y0=5, x1=\"2020-10-07T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","fig.add_shape(type=\"line\",\n","    x0=\"2020-10-15T00:00:00\", y0=5, x1=\"2020-10-15T00:00:00\", y1=30,\n","    line=dict(color=\"black\", width=1, dash=\"dashdot\")\n",")\n","\n","fig.add_annotation(x=\"2020-10-07T00:00:00\", y=25,\n","            text=\"Anomaly period 2\",\n","            showarrow=True,\n","            arrowhead=1,\n","            ax=70,\n","            ay=-50)\n","\n","fig.add_annotation(x=\"2020-09-15T00:00:00\", y=25,\n","            text=\"Anomaly period 1\",\n","            showarrow=True,\n","            arrowhead=1,\n","            ax=60,\n","            ay=-50)\n","\n","\n","\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHa3MAb25p8a"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpob0E15v-WD"},"source":["df_outliers = pd.DataFrame(columns=df_transformer_loss.columns)\r\n","for key in outlier_time.keys():\r\n","  time_period = outlier_time[key]\r\n","  #print(time_period)\r\n","\r\n","  for model, name in [(df_gru_loss_plot,'GRU AE'),(df_test_loss_deepant,'DeepAnT'),(df_transformer_loss,'Transformer')]:\r\n","    #for t in time_period:\r\n","      #print(t)\r\n","      df_time = model[model.sensor_id == int(key)]\r\n","      #df_time = model.loc[t[0]:t[1]].copy()\r\n","      #df_time = model[model.sensor_id == int(key)]\r\n","\r\n","      #df_time['outlier_period'] = [str(t[0])[:10] + ' - ' + str(t[1])[:10] for i in range(len(df_time))]\r\n","\r\n","      #df_time['outlier_period'] = 'sensor ' + df_time['sensor_id'].astype(str) + ':  ' + df_time['outlier_period'].astype(str)\r\n","\r\n","      df_time['Outlier'] = df_time['euclidean_dist'] > df_time['threshold_euc']\r\n","\r\n","      df_time['Model'] = name\r\n","      df_time['length_of_period'] = int(len(df_time))\r\n","\r\n","      df_outliers = df_outliers.append(df_time)\r\n","\r\n","df_outliers.drop(columns=['index','time_periods','time-period'])\r\n","df_outliers['thres_euc_dif'] = df_outliers['euclidean_dist'] - df_outliers['threshold_euc']\r\n","\r\n","df_outliers = df_outliers[['Model','sensor_id','timestamp','length_of_period','Outlier','thres_euc_dif','euclidean_dist','threshold_euc','loss',\r\n","                           'loss_temp','loss_humid','loss_moist','true_temp','true_humid','true_moist',\r\n","                           'pred_temp','pred_humid','pred_moist','threshold_temp','threshold_humid','threshold_moist']]\r\n","\r\n","sub_df_outliers= df_outliers[df_outliers['sensor_id']==sensor]\r\n","\r\n","sub_df_outliers['Outlier'] = sub_df_outliers['Outlier'].astype(int)\r\n","sub_df_outliers['timestamp'] = pd.to_datetime(sub_df_outliers['timestamp'],infer_datetime_format=True,utc=True).dt.date\r\n","SUM = sub_df_outliers[['timestamp','Model','Outlier']].groupby(['Model','timestamp']).sum().reset_index()\r\n","import plotly.graph_objects as go\r\n","import datetime\r\n","import numpy as np\r\n","\r\n","sensors = sub_df_outliers['sensor_id'].unique()\r\n","\r\n","z = SUM['Outlier'].values\r\n","date = SUM['timestamp'].values\r\n","\r\n","fig = go.Figure(data=go.Heatmap(\r\n","        z=z,\r\n","        x=date,\r\n","        y=SUM['Model'].values,\r\n","        colorscale='OrRd'))\r\n","\"\"\"\r\n","fig = go.Figure(data=go.Heatmap(\r\n","        z=z,\r\n","        x=date,\r\n","        y=SUM['Model'].values,\r\n","        colorscale='OrRd'))\r\n","\"\"\"\r\n","fig.update_layout(\r\n","    title='Anomalies per day on selected sensor',\r\n","    xaxis_nticks=36,\r\n","    #height = 700,\r\n","    width = 1920\r\n","    )\r\n","\r\n","fig.update_layout(plot_bgcolor='rgb(255,247,236)')\r\n","fig.update_layout(xaxis_showgrid=False, yaxis_showgrid=False)\r\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"horTAC2Jwscr"},"source":[""],"execution_count":null,"outputs":[]}]}